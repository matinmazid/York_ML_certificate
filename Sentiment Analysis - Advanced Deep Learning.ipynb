{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sd_X_0u1-zd"
      },
      "source": [
        "# Import necessary depencencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install text_normalizer==0.1.3\n",
        "!pip install HTMLParser==0.0.2\n",
        "\n",
        "!wget https://raw.githubusercontent.com/dipanjanS/practical-machine-learning-with-python/master/notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment/model_evaluation_utils.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YDjtBN_3W3X",
        "outputId": "f5abb6e1-1946-411b-d45e-490fe3bebc9d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: text_normalizer==0.1.3 in /usr/local/lib/python3.10/dist-packages (0.1.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from text_normalizer==0.1.3) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->text_normalizer==0.1.3) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->text_normalizer==0.1.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->text_normalizer==0.1.3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->text_normalizer==0.1.3) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->text_normalizer==0.1.3) (1.17.0)\n",
            "--2024-12-14 01:55:31--  https://raw.githubusercontent.com/dipanjanS/practical-machine-learning-with-python/master/notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment/model_evaluation_utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8575 (8.4K) [text/plain]\n",
            "Saving to: ‘model_evaluation_utils.py’\n",
            "\n",
            "model_evaluation_ut 100%[===================>]   8.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-14 01:55:31 (51.9 MB/s) - ‘model_evaluation_utils.py’ saved [8575/8575]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgf7uCI-2Fmw",
        "outputId": "f0268508-3fc0-4614-85bc-2d90c82e567e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "18zqZA0f1-zf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import text_normalizer as tn\n",
        "import model_evaluation_utils as meu\n",
        "\n",
        "np.set_printoptions(precision=2, linewidth=80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VYS7AiO1-zg"
      },
      "source": [
        "# Load and normalize data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import html.parser\n",
        "import unicodedata\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')\n",
        "\n",
        "CONTRACTION_MAP = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n",
        "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
        "                     accented_char_removal=True, text_lower_case=True,\n",
        "                     text_lemmatization=True, special_char_removal=True,\n",
        "                     stopword_removal=True):\n",
        "\n",
        "    normalized_corpus = []\n",
        "\n",
        "    for doc in corpus:\n",
        "\n",
        "        if html_stripping:\n",
        "            doc = strip_html_tags(doc)\n",
        "\n",
        "        if accented_char_removal:\n",
        "            doc = remove_accented_chars(doc)\n",
        "\n",
        "        if contraction_expansion:\n",
        "            doc = expand_contractions(doc)\n",
        "\n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "\n",
        "        # remove extra newlines\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        # insert spaces between special characters to isolate them\n",
        "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "\n",
        "        if text_lemmatization:\n",
        "            doc = lemmatize_text(doc)\n",
        "\n",
        "        if special_char_removal:\n",
        "            doc = remove_special_characters(doc)\n",
        "\n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
        "\n",
        "        normalized_corpus.append(doc)\n",
        "\n",
        "    return normalized_corpus\n",
        "\n",
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "\n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "def remove_special_characters(text):\n",
        "    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "# # Cleaning Text - strip HTML\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    return stripped_text\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aELy2_mSABzZ",
        "outputId": "a862afc2-f662-4c8e-d0cc-15c46efb9d53"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "jJoBV44U1-zg",
        "outputId": "a3b5bec1-d8ea-4eaa-e674-a7ef3c0d739c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'remove_accented_chars' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-b222bd210764>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# normalize datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mnorm_train_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mnorm_test_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-bd8c36808dac>\u001b[0m in \u001b[0;36mnormalize_corpus\u001b[0;34m(corpus, html_stripping, contraction_expansion, accented_char_removal, text_lower_case, text_lemmatization, special_char_removal, stopword_removal)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maccented_char_removal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_accented_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontraction_expansion\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'remove_accented_chars' is not defined"
          ]
        }
      ],
      "source": [
        "import html.parser\n",
        "dataset = pd.read_csv(r'/content/drive/MyDrive/ColabNotebooks/ml_1002/movie_reviews.csv')\n",
        "\n",
        "# take a peek at the data\n",
        "print(dataset.head())\n",
        "reviews = np.array(dataset['review'])\n",
        "sentiments = np.array(dataset['sentiment'])\n",
        "\n",
        "# build train and test datasets\n",
        "train_reviews = reviews[:35000]\n",
        "train_sentiments = sentiments[:35000]\n",
        "test_reviews = reviews[35000:]\n",
        "test_sentiments = sentiments[35000:]\n",
        "\n",
        "# normalize datasets\n",
        "norm_train_reviews = normalize_corpus(train_reviews)\n",
        "norm_test_reviews = normalize_corpus(test_reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VZ1aYrK1-zh"
      },
      "source": [
        "# Tokenize train & test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "EA2Fpocp1-zh",
        "outputId": "64a00f62-5d32-4887-d026-3fe5f9e3228b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'norm_train_reviews' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b72592f88a8d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnorm_train_reviews\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenized_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnorm_test_reviews\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'norm_train_reviews' is not defined"
          ]
        }
      ],
      "source": [
        "tokenized_train = [tn.tokenizer.tokenize(text) for text in norm_train_reviews]\n",
        "tokenized_test = [tn.tokenizer.tokenize(text) for text in norm_test_reviews]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQITEa061-zh"
      },
      "source": [
        "# Build Vocabulary Mapping (word to index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "U_xbkidN1-zh",
        "outputId": "cafe3f64-c222-47a4-a6ec-c4ae452490a5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenized_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1332bcae752b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# build word to index vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtoken_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_train\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvocab_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmax_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenized_train' is not defined"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# build word to index vocabulary\n",
        "token_counter = Counter([token for review in tokenized_train for token in review])\n",
        "vocab_map = {item[0]: index+1 for index, item in enumerate(dict(token_counter).items())}\n",
        "max_index = np.max(list(vocab_map.values()))\n",
        "vocab_map['PAD_INDEX'] = 0\n",
        "vocab_map['NOT_FOUND_INDEX'] = max_index+1\n",
        "vocab_size = len(vocab_map)\n",
        "# view vocabulary size and part of the vocabulary map\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Sample slice of vocabulary map:', dict(list(vocab_map.items())[10:20]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJthW9SU1-zi"
      },
      "source": [
        "# Encode and Pad datasets & Encode prediction class labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "pl9kDafO1-zi",
        "outputId": "b83e5a9d-ef09-4298-d43a-27e1bc4f44b0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenized_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-80a852724449>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;31m# positive -> 1, negative -> 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m## Train reviews data corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenized_train' is not defined"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing import sequence\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# get max length of train corpus and initialize label encoder\n",
        "le = LabelEncoder()\n",
        "num_classes=2 # positive -> 1, negative -> 0\n",
        "max_len = np.max([len(review) for review in tokenized_train])\n",
        "\n",
        "## Train reviews data corpus\n",
        "# Convert tokenized text reviews to numeric vectors\n",
        "train_X = [[vocab_map[token] for token in tokenized_review] for tokenized_review in tokenized_train]\n",
        "train_X = sequence.pad_sequences(train_X, maxlen=max_len) # pad\n",
        "## Train prediction class labels\n",
        "# Convert text sentiment labels (negative\\positive) to binary encodings (0/1)\n",
        "train_y = le.fit_transform(train_sentiments)\n",
        "\n",
        "## Test reviews data corpus\n",
        "# Convert tokenized text reviews to numeric vectors\n",
        "test_X = [[vocab_map[token] if vocab_map.get(token) else vocab_map['NOT_FOUND_INDEX']\n",
        "           for token in tokenized_review]\n",
        "              for tokenized_review in tokenized_test]\n",
        "test_X = sequence.pad_sequences(test_X, maxlen=max_len)\n",
        "## Test prediction class labels\n",
        "# Convert text sentiment labels (negative\\positive) to binary encodings (0/1)\n",
        "test_y = le.transform(test_sentiments)\n",
        "\n",
        "# view vector shapes\n",
        "print('Max length of train review vectors:', max_len)\n",
        "print('Train review vectors shape:', train_X.shape, ' Test review vectors shape:', test_X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJYqXXAf1-zi"
      },
      "source": [
        "# Build the LSTM Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Vsik_9t51-zi",
        "outputId": "10913340-8e95-4485-b879-48d7503e3571"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vocab_size' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b34bd2d95a6b>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpatialDropout1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Dropout, SpatialDropout1D\n",
        "from keras.layers import LSTM\n",
        "\n",
        "EMBEDDING_DIM = 128 # dimension for dense embeddings for each token\n",
        "LSTM_DIM = 64 # total LSTM units\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(LSTM_DIM, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBzqJAP_1-zi",
        "outputId": "88ec9ff0-95aa-4076-fdd0-2208c50cdbbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 1442, 128)         10541824  \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_4 (Spatial (None, 1442, 128)         0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 10,591,297\n",
            "Trainable params: 10,591,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PGOrt-b1-zi"
      },
      "source": [
        "# Visualize model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riZe6YdR1-zi",
        "outputId": "8af553bb-4301-4fdb-ce5e-e7e63213c0e8"
      },
      "outputs": [
        {
          "data": {
            "image/svg+xml": [
              "<svg height=\"78pt\" viewBox=\"0.00 0.00 1118.00 78.00\" width=\"1118pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
              "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 74)\">\n",
              "<title>G</title>\n",
              "<polygon fill=\"white\" points=\"-4,4 -4,-74 1114,-74 1114,4 -4,4\" stroke=\"none\"/>\n",
              "<!-- 2972249086720 -->\n",
              "<g class=\"node\" id=\"node1\"><title>2972249086720</title>\n",
              "<polygon fill=\"none\" points=\"0,-0.5 0,-69.5 180,-69.5 180,-0.5 0,-0.5\" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-54.3\">InputLayer</text>\n",
              "<polyline fill=\"none\" points=\"0,-46.5 180,-46.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"42.5\" y=\"-31.3\">input:</text>\n",
              "<polyline fill=\"none\" points=\"85,-23.5 85,-46.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"132.5\" y=\"-31.3\">output:</text>\n",
              "<polyline fill=\"none\" points=\"0,-23.5 180,-23.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"45\" y=\"-8.3\">(None, 1442)</text>\n",
              "<polyline fill=\"none\" points=\"90,-0.5 90,-23.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135\" y=\"-8.3\">(None, 1442)</text>\n",
              "</g>\n",
              "<!-- 2972249086552 -->\n",
              "<g class=\"node\" id=\"node2\"><title>2972249086552</title>\n",
              "<polygon fill=\"none\" points=\"216,-0.5 216,-69.5 424,-69.5 424,-0.5 216,-0.5\" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"320\" y=\"-54.3\">Embedding</text>\n",
              "<polyline fill=\"none\" points=\"216,-46.5 424,-46.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"265.5\" y=\"-31.3\">input:</text>\n",
              "<polyline fill=\"none\" points=\"315,-23.5 315,-46.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"369.5\" y=\"-31.3\">output:</text>\n",
              "<polyline fill=\"none\" points=\"216,-23.5 424,-23.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"261\" y=\"-8.3\">(None, 1442)</text>\n",
              "<polyline fill=\"none\" points=\"306,-0.5 306,-23.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"365\" y=\"-8.3\">(None, 1442, 128)</text>\n",
              "</g>\n",
              "<!-- 2972249086720&#45;&gt;2972249086552 -->\n",
              "<g class=\"edge\" id=\"edge1\"><title>2972249086720-&gt;2972249086552</title>\n",
              "<path d=\"M180.13,-35C188.569,-35 197.198,-35 205.821,-35\" fill=\"none\" stroke=\"black\"/>\n",
              "<polygon fill=\"black\" points=\"205.93,-38.5001 215.93,-35 205.93,-31.5001 205.93,-38.5001\" stroke=\"black\"/>\n",
              "</g>\n",
              "<!-- 2972249086608 -->\n",
              "<g class=\"node\" id=\"node3\"><title>2972249086608</title>\n",
              "<polygon fill=\"none\" points=\"460,-0.5 460,-69.5 696,-69.5 696,-0.5 460,-0.5\" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"578\" y=\"-54.3\">SpatialDropout1D</text>\n",
              "<polyline fill=\"none\" points=\"460,-46.5 696,-46.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"516.5\" y=\"-31.3\">input:</text>\n",
              "<polyline fill=\"none\" points=\"573,-23.5 573,-46.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"634.5\" y=\"-31.3\">output:</text>\n",
              "<polyline fill=\"none\" points=\"460,-23.5 696,-23.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"519\" y=\"-8.3\">(None, 1442, 128)</text>\n",
              "<polyline fill=\"none\" points=\"578,-0.5 578,-23.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"637\" y=\"-8.3\">(None, 1442, 128)</text>\n",
              "</g>\n",
              "<!-- 2972249086552&#45;&gt;2972249086608 -->\n",
              "<g class=\"edge\" id=\"edge2\"><title>2972249086552-&gt;2972249086608</title>\n",
              "<path d=\"M424.018,-35C432.507,-35 441.147,-35 449.783,-35\" fill=\"none\" stroke=\"black\"/>\n",
              "<polygon fill=\"black\" points=\"449.909,-38.5001 459.909,-35 449.909,-31.5001 449.909,-38.5001\" stroke=\"black\"/>\n",
              "</g>\n",
              "<!-- 2972249087392 -->\n",
              "<g class=\"node\" id=\"node4\"><title>2972249087392</title>\n",
              "<polygon fill=\"none\" points=\"732,-0.5 732,-69.5 927,-69.5 927,-0.5 732,-0.5\" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"829.5\" y=\"-54.3\">LSTM</text>\n",
              "<polyline fill=\"none\" points=\"732,-46.5 927,-46.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"778.5\" y=\"-31.3\">input:</text>\n",
              "<polyline fill=\"none\" points=\"825,-23.5 825,-46.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"876\" y=\"-31.3\">output:</text>\n",
              "<polyline fill=\"none\" points=\"732,-23.5 927,-23.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"791\" y=\"-8.3\">(None, 1442, 128)</text>\n",
              "<polyline fill=\"none\" points=\"850,-0.5 850,-23.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"888.5\" y=\"-8.3\">(None, 64)</text>\n",
              "</g>\n",
              "<!-- 2972249086608&#45;&gt;2972249087392 -->\n",
              "<g class=\"edge\" id=\"edge3\"><title>2972249086608-&gt;2972249087392</title>\n",
              "<path d=\"M696.079,-35C704.724,-35 713.419,-35 721.995,-35\" fill=\"none\" stroke=\"black\"/>\n",
              "<polygon fill=\"black\" points=\"722,-38.5001 732,-35 722,-31.5001 722,-38.5001\" stroke=\"black\"/>\n",
              "</g>\n",
              "<!-- 2972249154672 -->\n",
              "<g class=\"node\" id=\"node5\"><title>2972249154672</title>\n",
              "<polygon fill=\"none\" points=\"963,-0.5 963,-69.5 1110,-69.5 1110,-0.5 963,-0.5\" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1036.5\" y=\"-54.3\">Dense</text>\n",
              "<polyline fill=\"none\" points=\"963,-46.5 1110,-46.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"997.5\" y=\"-31.3\">input:</text>\n",
              "<polyline fill=\"none\" points=\"1032,-23.5 1032,-46.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1071\" y=\"-31.3\">output:</text>\n",
              "<polyline fill=\"none\" points=\"963,-23.5 1110,-23.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1001.5\" y=\"-8.3\">(None, 64)</text>\n",
              "<polyline fill=\"none\" points=\"1040,-0.5 1040,-23.5 \" stroke=\"black\"/>\n",
              "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1075\" y=\"-8.3\">(None, 1)</text>\n",
              "</g>\n",
              "<!-- 2972249087392&#45;&gt;2972249154672 -->\n",
              "<g class=\"edge\" id=\"edge4\"><title>2972249087392-&gt;2972249154672</title>\n",
              "<path d=\"M927.297,-35C935.773,-35 944.301,-35 952.648,-35\" fill=\"none\" stroke=\"black\"/>\n",
              "<polygon fill=\"black\" points=\"952.725,-38.5001 962.725,-35 952.725,-31.5001 952.725,-38.5001\" stroke=\"black\"/>\n",
              "</g>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False,\n",
        "                 rankdir='LR').create(prog='dot', format='svg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1J4fh3v1-zi"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIOA0JI31-zi",
        "outputId": "c99d9fac-6990-4b3b-a8c3-ebec695fb18a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 31500 samples, validate on 3500 samples\n",
            "Epoch 1/5\n",
            "31500/31500 [==============================] - 2491s - loss: 0.4081 - acc: 0.8184 - val_loss: 0.3006 - val_acc: 0.8751\n",
            "Epoch 2/5\n",
            "31500/31500 [==============================] - 2489s - loss: 0.2253 - acc: 0.9158 - val_loss: 0.3209 - val_acc: 0.8780\n",
            "Epoch 3/5\n",
            "31500/31500 [==============================] - 2656s - loss: 0.1431 - acc: 0.9493 - val_loss: 0.3483 - val_acc: 0.8671\n",
            "Epoch 4/5\n",
            "31500/31500 [==============================] - 2604s - loss: 0.1023 - acc: 0.9658 - val_loss: 0.3803 - val_acc: 0.8729\n",
            "Epoch 5/5\n",
            "31500/31500 [==============================] - 2701s - loss: 0.0694 - acc: 0.9761 - val_loss: 0.4430 - val_acc: 0.8706\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2b411229e80>"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 100\n",
        "model.fit(train_X, train_y, epochs=5, batch_size=batch_size,\n",
        "          shuffle=True, validation_split=0.1, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr-ciKjT1-zj"
      },
      "source": [
        "# Predict and Evaluate Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL8aI0kf1-zj",
        "outputId": "e45a983d-0f43-4c7b-ab41-6eb077c7776d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15000/15000 [==============================] - 352s   \n"
          ]
        }
      ],
      "source": [
        "pred_test = model.predict_classes(test_X)\n",
        "predictions = le.inverse_transform(pred_test.flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQzGBcgE1-zj",
        "outputId": "be291cd2-9863-423f-de1a-a7ba4d743e15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Performance metrics:\n",
            "------------------------------\n",
            "Accuracy: 0.88\n",
            "Precision: 0.88\n",
            "Recall: 0.88\n",
            "F1 Score: 0.88\n",
            "\n",
            "Model Classification report:\n",
            "------------------------------\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "   positive       0.87      0.88      0.88      7510\n",
            "   negative       0.88      0.87      0.88      7490\n",
            "\n",
            "avg / total       0.88      0.88      0.88     15000\n",
            "\n",
            "\n",
            "Prediction Confusion Matrix:\n",
            "------------------------------\n",
            "                 Predicted:         \n",
            "                   positive negative\n",
            "Actual: positive       6633      877\n",
            "        negative        972     6518\n"
          ]
        }
      ],
      "source": [
        "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions,\n",
        "                                      classes=['positive', 'negative'])"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}