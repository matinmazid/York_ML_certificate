{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9iNhb52i96mEynBVOw9nj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matinmazid/York_ML_certificate/blob/master/assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load all the dependant libraries"
      ],
      "metadata": {
        "id": "9ZXkXCB8MvvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario 0 No context provided to model"
      ],
      "metadata": {
        "id": "p4mpG5Ve7zpt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5BUlQT1VKP0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "523e0ada-8a5b-4689-df35-c67e2cf8cd46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip --quiet install langchain langchain_community pypdf langchain_core langchain-text-splitters langchain-chroma\n",
        "!pip --quiet install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY']=userdata.get('OPEN_API')"
      ],
      "metadata": {
        "id": "1Z0NJOq4NBF_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the lang chain"
      ],
      "metadata": {
        "id": "Dxu-lUg_UHyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "import langchain_core\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "model = ChatOpenAI(temperature=0, model_name = \"gpt-3.5-turbo\")\n",
        "output_parser = StrOutputParser()\n",
        "chatPrompt=ChatPromptTemplate.from_messages(\n",
        "    [(\"system\",\"you are a financial specialist with experience in Aritifical Intelligence\"),\n",
        "     (\"user\",\"what is XBRL and its relationship to AICPA\")]\n",
        "    )"
      ],
      "metadata": {
        "id": "9V5-5Df1UEFY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain= chatPrompt | model | output_parser"
      ],
      "metadata": {
        "id": "5HDEo4-hUa1I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "MDYGytCIiZfi",
        "outputId": "041c1aea-b74f-4ebf-d2dc-77557f20ebbb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'XBRL stands for eXtensible Business Reporting Language, which is a standardized format for financial reporting. It allows companies to tag financial data in their reports so that it can be easily analyzed and compared across different organizations. XBRL helps to streamline the process of financial reporting and makes it more efficient and accurate.\\n\\nThe American Institute of Certified Public Accountants (AICPA) is a professional organization for accountants in the United States. The AICPA has been involved in promoting the use of XBRL for financial reporting. They have developed guidelines and best practices for using XBRL in financial statements to ensure consistency and accuracy in reporting.\\n\\nIn summary, XBRL is a standardized format for financial reporting, and the AICPA has been actively involved in promoting its use and providing guidance on how to implement it effectively in financial reporting processes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario 1 Load pdf document loader saving to Chromadb"
      ],
      "metadata": {
        "id": "3itOPpD27ak5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet pypdf\n",
        "!pip install --upgrade --quiet langchain_openai\n",
        "!pip install --quiet tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH4Rs-Tbji3p",
        "outputId": "133728cf-2290-438e-82de-d1cd0197dfaa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.stern.nyu.edu/sites/default/files/assets/documents/ChenChoDouLev2021WP.pdf\n",
        "\n",
        "https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/"
      ],
      "metadata": {
        "id": "cHc6DHAXg7vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "import tiktoken\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "loader = PyPDFLoader(\"ChenChoDouLev2021WP.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "\n",
        "# split and chuck the input data\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
        "# text splitter knows how to extract the text of a sourced doc\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# all_texts = [cleanStr(doc.page_content) for doc in all_splits]\n",
        "# all_texts\n",
        "\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits,\n",
        "                                    embedding=OpenAIEmbeddings())\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "from langchain import hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"what is XBRL and its relationship to AICPA\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "Ss4xJraiBFzd",
        "outputId": "02158daa-0be6-4730-cbe0-730b3aae08ba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"XBRL is the eXtensible Business Reporting Language used for financial reporting. The AICPA is involved in developing data quality tools and working with the FASB on U.S. GAAP taxonomy enhancements related to XBRL. Plumlee and Plumlee (2008) and Boritz and No (2009) discuss the potential challenges of XBRL documents' assurance.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario 2 Pine cone database and webcrawler\n",
        "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/\n",
        "https://www150.statcan.gc.ca/n1/daily-quotidien/250207/dq250207a-eng.htm\n"
      ],
      "metadata": {
        "id": "XSPrIYkqo_fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-community beautifulsoup4 lxml"
      ],
      "metadata": {
        "id": "GsQODv-spVrc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "# import html as htmlTool\n",
        "\n",
        "\n",
        "def bs4_extractor(html: str) -> str:\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    someText=re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n",
        "    # someText=htmlTool.unescape(someText)\n",
        "    # the test page had non-breaking space as well as other non acii characters\n",
        "    # we get rid of them\n",
        "    return someText.encode('latin-1', 'ignore').decode('ascii',\"ignore\")\n"
      ],
      "metadata": {
        "id": "4S8r8Y7duVgS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import RecursiveUrlLoader\n",
        "\n",
        "loader=RecursiveUrlLoader(url=\"https://www150.statcan.gc.ca/n1/daily-quotidien/250207/dq250207a-eng.htm\",\n",
        "                           use_async=False\n",
        "                          ,extractor=bs4_extractor)\n",
        "\n",
        "docs_html = loader.load()\n",
        "\n",
        "# text splitter knows how to extract the text of a sourced doc\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
        "# text splitter knows how to extract the text of a sourced doc\n",
        "all_splits = text_splitter.split_documents(docs_html)\n"
      ],
      "metadata": {
        "id": "61wtdTFmtGn1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocF2mfp1GYVX",
        "outputId": "a2db8d1a-428d-418d-f577-6b365d8f42fe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://www150.statcan.gc.ca/n1/daily-quotidien/250207/dq250207a-eng.htm', 'content_type': 'text/html; charset=UTF-8', 'title': 'The Daily\\xa0—\\xa0Labour Force Survey, January 2025', 'description': 'Employment increased by 76,000 (+0.4%) in January and the employment rate rose 0.1 percentage points to 61.1%. The unemployment rate declined 0.1 percentage points to 6.6%.', 'language': 'en'}, page_content='527,000\\nJanuary2025\\n\\n-0.1%\\n(monthly change)\\n\\n Source(s): Table 14-10-0287-01.\\r\\n\\n\\r\\nUnemployment rate  N.S.\\r\\n\\n5.9%\\nJanuary2025\\n\\n-0.4pts\\n(monthly change)\\n\\n Source(s): Table 14-10-0287-01.\\r\\n\\nTab1\\nTab2\\n\\n\\r\\nEmployment  N.B.\\r\\n\\n403,000\\nJanuary2025\\n\\n0.7%\\n(monthly change)\\n\\n Source(s): Table 14-10-0287-01.\\r\\n\\n\\r\\nUnemployment rate  N.B.\\r\\n\\n6.4%\\nJanuary2025\\n\\n-1.3pts\\n(monthly change)\\n\\n Source(s): Table 14-10-0287-01.\\r\\n\\nTab1\\nTab2\\n\\n\\r\\nEmployment  Que.\\r\\n\\n4,636,000\\nJanuary2025\\n\\n0.3%\\n(monthly change)')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WtbsukXYGwe8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pinecone requires a API key.\n",
        "https://docs.pinecone.io/guides/get-started/quickstart"
      ],
      "metadata": {
        "id": "Hq3Jkg5554aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet protobuf==3.20.*\n",
        "!pip install  --quiet \"pinecone[grpc]\"\n",
        "!pip install --upgrade --quiet  lark\n",
        "!pip install --upgrade --quiet pinecone-notebooks pinecone-client\n",
        "!pip install --upgrade --quiet langchain_pinecone\n"
      ],
      "metadata": {
        "id": "dRd-r8fs4EsG",
        "outputId": "6e3e11db-0a42-4a44-9efb-e3a5728098d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/162.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m143.4/162.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-proto 1.30.0 requires protobuf<6.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.4/421.4 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone.grpc import PineconeGRPC as Pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "import time\n",
        "\n",
        "pc = Pinecone(api_key=userdata.get('PINECONE'))\n",
        "os.environ['PINECONE_API_KEY']=userdata.get('PINECONE')"
      ],
      "metadata": {
        "id": "_tHvGn6x6kUa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "index_name = \"york-1020-assignment-2\"\n",
        "# create new index\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "\n",
        "\n",
        "pinecone_vector_store= PineconeVectorStore.from_documents(\n",
        "    documents=all_splits,\n",
        "    embedding=OpenAIEmbeddings(),\n",
        "    index_name=index_name,\n",
        "    text_key=\"page_content\"\n",
        ")\n",
        "\n",
        "pinecone_retriever = pinecone_vector_store.as_retriever()\n",
        "#\n",
        "pinecone_rag_chain = (\n",
        "    {\"context\": pinecone_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "Q6RBxpCo_uvz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone_rag_chain.invoke(\"how much has employment increased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_dwD6XdHLdTE",
        "outputId": "2e232b78-cfef-425c-8625-49df9b583973"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Employment increased by 76,000 (+0.4%) in January, with the employment rate rising by 0.1 percentage points to 61.1%. The recent increases in employment follow a period where growth had been outpaced by population growth.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario 3 drop box as cloud storage and weaviate"
      ],
      "metadata": {
        "id": "O2lt6r36L6d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet dropbox\n",
        "!pip install --quiet \"unstructured[pdf]\"\n",
        "!pip install -U --quiet weaviate-client\n",
        "!pip install --qU unstructured"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIl0gnkgOTa_",
        "outputId": "626ef97d-f55d-4ce4-d7d5-a8b2a7e0e04d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --qU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://python.langchain.com/docs/integrations/document_loaders/dropbox/\n",
        "https://python.langchain.com/docs/integrations/vectorstores/weaviate/#connecting-to-weaviate\n",
        "https://weaviate.io/developers/weaviate/quickstart/local\n"
      ],
      "metadata": {
        "id": "2hreT3wojNZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DropboxLoader\n",
        "# Generate access token: https://www.dropbox.com/developers/apps/create.\n",
        "dropbox_access_token = userdata.get('DROP_BOX')\n",
        "# Dropbox root folder\n",
        "dropbox_folder_path = \"\"\n",
        "loader = DropboxLoader(\n",
        "    dropbox_access_token=dropbox_access_token,\n",
        "    dropbox_folder_path=dropbox_folder_path,\n",
        "    recursive=True,\n",
        ")"
      ],
      "metadata": {
        "id": "bHjMROAIQmbS"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = loader.load()\n"
      ],
      "metadata": {
        "id": "qs8TVRBKRJ7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046e4d17-fb49-4753-9f2c-34ec48148769"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File /trust-worthy-artificial-intelligence.pdf type detected as .pdf\n",
            "File /ai-future-trends.pdf type detected as .pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load weaveate\n",
        "https://weaviate.io/developers/weaviate/client-libraries/python"
      ],
      "metadata": {
        "id": "YIf6c7v26Qxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U weaviate-client\n",
        "!pip install -Uq langchain-weaviate\n",
        "!pip install openai tiktoken langchain"
      ],
      "metadata": {
        "id": "CR1R_FR-6Wnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "import os\n",
        "\n",
        "# Best practice: store your credentials in environment variables\n",
        "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "# this is not rerunable. Check error\n",
        "# this line seems to start the weaveate server\n",
        "weaviate_client = weaviate.connect_to_embedded(\n",
        "      version=\"1.26.1\",\n",
        "      headers={\n",
        "          \"X-OpenAI-Api-Key\": openai_api_key\n",
        "      },\n",
        "  )\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ft2BOYD76QRE",
        "outputId": "0db0fb08-22fa-426f-998f-f030c0e5d612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "WeaviateStartUpError",
          "evalue": "Embedded DB did not start because processes are already listening on ports http:8079 and grpc:50050use weaviate.connect_to_local(port=8079, grpc_port=50050) to connect to the existing instance",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mWeaviateStartUpError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-861d1581d935>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# this is not rerunable. Check error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m weaviate_client = weaviate.connect_to_embedded(\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1.26.1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       headers={\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weaviate/connect/helpers.py\u001b[0m in \u001b[0;36mconnect_to_embedded\u001b[0;34m(hostname, port, grpc_port, headers, additional_config, version, persistence_data_path, binary_path, environment_variables)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbinary_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     client = WeaviateClient(\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0membedded_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0madditional_headers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weaviate/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, connection_params, embedded_options, auth_client_secret, additional_headers, additional_config, skip_init_checks)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mconnection_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0membedded_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedded_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weaviate/client_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, connection_params, embedded_options, auth_client_secret, additional_headers, additional_config, skip_init_checks)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \"\"\"\n\u001b[1;32m     68\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot initialize a WeaviateClient without an event loop.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         connection_params, embedded_db = self.__parse_connection_params_and_embedded_db(\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mconnection_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weaviate/client_base.py\u001b[0m in \u001b[0;36m__parse_connection_params_and_embedded_db\u001b[0;34m(self, connection_params, embedded_options)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0membedded_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddedV4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedded_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0membedded_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             return (\n\u001b[1;32m    110\u001b[0m                 ConnectionParams(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/weaviate/embedded.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_listening\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             raise WeaviateStartUpError(\n\u001b[0m\u001b[1;32m    322\u001b[0m                 \u001b[0;34mf\"Embedded DB did not start because processes are already listening on ports http:{self.options.port} and grpc:{self.grpc_port}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;34mf\"use weaviate.connect_to_local(port={self.options.port}, grpc_port={self.options.grpc_port}) to connect to the existing instance\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mWeaviateStartUpError\u001b[0m: Embedded DB did not start because processes are already listening on ports http:8079 and grpc:50050use weaviate.connect_to_local(port=8079, grpc_port=50050) to connect to the existing instance"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "PxdMNHVa8GIF"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
        "weaviate_client_local = weaviate.connect_to_local( host=\"localhost\", port=8079,\n",
        "                                                  grpc_port=50050,\n",
        "    headers={\n",
        "          \"X-OpenAI-Api-Key\": openai_api_key\n",
        "      },\n",
        ")\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = WeaviateVectorStore.from_documents(docs, embeddings, client=weaviate_client_local)"
      ],
      "metadata": {
        "id": "FCAkfdQ8-Zcs"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format_docs = lambda x: \"\\n\\n\".join([doc.page_content for doc in x])"
      ],
      "metadata": {
        "id": "q_qiZiUHDFaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weaviate_retriever = db.as_retriever()\n",
        "#\n",
        "weaviate_rag_chain = (\n",
        "    {\"context\": weaviate_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "Rm6RlHulBLlT"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weaviate_rag_chain.invoke(\"What are the different categories of AI\")"
      ],
      "metadata": {
        "id": "dniKu2E0EcB7",
        "outputId": "8ca5025e-68bb-49b3-e93c-17ffc30dd989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The different categories of AI are artificial narrow AI, general AI, and super AI. Narrow AI is commonly applied in industries like hospitality and tourism for specific tasks. General AI is currently in development and aims to perform tasks intelligently similar to human capabilities.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    }
  ]
}